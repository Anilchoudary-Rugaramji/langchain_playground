{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e35cf8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Ingestion or data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c49e59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speach.txt'}, page_content='\\n\\nSisters and Brothers of America,\\n\\nIt fills my heart with joy unspeakable to rise in response to the warm and cordial welcome you have given me. I thank you in the name of the most ancient order of monks in the world; I thank you in the name of the mother of religions; and I thank you in the name of millions and millions of Hindu people of all classes and sects.\\n\\nI am proud to belong to a religion which has taught the world both tolerance and universal acceptance. We believe not only in universal tolerance but we accept all religions as true. I am proud to belong to a nation which has sheltered the persecuted and the refugees of all religions and all nations of the earth. I am proud to tell you that we have gathered in our bosom the purest remnant of the Israelites, who came to southern India and took refuge with us when their holy temple was shattered to pieces by Roman tyranny. I am proud to belong to the religion which has sheltered and is still fostering the remnant of the grand Zoroastrian nation.\\n\\nI will quote to you, brethren, a few lines from a hymn which I remember to have repeated from my earliest boyhood: “As the different streams having their sources in different places all mingle their water in the sea, so, O Lord, the different paths which men take, through different tendencies, various though they appear, crooked or straight, all lead to Thee.”\\n\\nThe present convention, which is one of the most august assemblies ever held, is in itself a vindication of the wonderful doctrine preached in the Gita: \"Whosoever comes to Me, through whatever form, I reach him; all men are struggling through paths which in the end lead to Me.\"\\n\\nSectarianism, bigotry, and its horrible descendant, fanaticism, have long possessed this beautiful earth. They have filled the earth with violence, drenched it often and often with human blood, destroyed civilization, and sent whole nations to despair. Had it not been for these terrible demons, human society would be far more advanced than it is now.\\n\\nI fervently hope that the bell that tolled this morning in honor of this convention may be the death-knell of all fanaticism, of all persecutions with the sword or with the pen, and of all uncharitable feelings between persons wending their way to the same goal.\\n\\nFrom the high spiritual plane of the Vedanta, we see the same truth reflected in all religions. We do not merely tolerate other religions, we accept them all as true. The Hindu may have failed to conquer the world with the sword, but he did so with spirituality, embracing unity in diversity and harmony in difference.\\n\\nLet this gathering be a beginning, a spiritual bond of humanity, a witness to the power of love, unity, and truth.\\n\\n---\\n\\nWould you like this version formatted as a printable speech or poster?\\n')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text loader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "loader = TextLoader(\"speach.txt\", encoding=\"utf-8\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54e63a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anilchoudary R\\Gitrepos\\langchain_playground\\myenv\\Lib\\site-packages\\pypdf\\generic\\_data_structures.py:680: RuntimeWarning: coroutine 'WebBaseLoader.fetch_all' was never awaited\n",
      "  retval = DictionaryObject()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 0, 'page_label': '1'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 1/35\\nWhy Pandas\\n1. Pandas library provides high perfromance usable data structures and data analysis tools\\nfor managing data tables \\n2. Supports standard functions to create pivot tables, column or row groupings, plotting\\ngraphs, joining tables like SQL, etc. \\nBusiness UseCase:\\nLet assume that you are working on an airlines data and you want are recieving data\\ncontinuously in the form of SQL tables or excel spreadsheets. You have created a detailed report\\nfrom the data and the management really likes the key insights you are deriving from data. They\\nwant you to send this on the weekly level to the entire team.\\nYou can either use SQL or excel spreadsheets to sit and create this every week or you can\\nautomate the task with a python script using pandas APIs. You can even automate sending the\\nemail with python.\\nWhat is Pandas\\nThere are three basic data structures in the pandas library:\\n1. DataFrame: This is the main data structure and it is a 2D table, similar to excel/spreadsheet\\ntables with columns names and row labels. \\n2. Series: It is a 1D array, similar to a column in excel or excel spreadsheet with column name\\nand row labels.\\n3. Panel : It is dictionary of dataframes. It is generally not used in practice and we will not be\\ndiscussing it in the videos.\\nPandas Series\\n1. Similar to python list for definition purpose. \\n2. For operations purpose, it behaves similar to numpy one-dimensional ndarrays and can be\\npassed directly to numpy functions.\\nCreating a Series\\nIn [3]:\\nimport pandas as pd \\nimport numpy as np \\nIn [2]:\\n# Using Python list as series objects \\ns = pd.Series([-1.,-1,21,5])'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 1, 'page_label': '2'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 2/35\\n0    -1.0 \\n1    -1.0 \\n2    21.0 \\n3     5.0 \\ndtype: float64\\nChuck     40 \\nDarwin    40 \\nElijah    40 \\ndtype: int64\\nNewton    6 \\nChuck     3 \\nDarwin    8 \\nElijah    9 \\ndtype: int64\\nChuck     3 \\nElijah    9 \\ndtype: int64\\n0     1 \\n1     2 \\n2    34 \\n3     4 \\ndtype: int64\\nCharles    6.0 \\nChuck     -5.4 \\nName: heights, dtype: float64\\nOperations on Series\\ns \\nOut[2]:\\nIn [3]:\\n# Using python list as index labels and scalar object values \\n \\ns = pd.Series(40, [\"Chuck\", \"Darwin\", \"Elijah\"]) \\ns \\nOut[3]:\\nIn [4]:\\n# Using dictionary with indecies \\ndict1 = {\"Newton\": 6, \"Chuck\": 3, \"Darwin\": 8, \"Elijah\": 9} \\ns = pd.Series(dict1) \\ns \\nOut[4]:\\nIn [5]:\\n#We can also specify the indicies explicity to control what goes into the series  \\ns = pd.Series(dict1, index = [\"Chuck\", \"Elijah\"]) \\ns \\nOut[5]:\\nIn [6]:\\n# Using numpy array \\n#s = pd.Series(np.array([[1,2],[34,4]])) \\ns = pd.Series(np.array([1,2,34,4])) \\ns \\nOut[6]:\\nIn [7]:\\n# Giving name to the series as  \\ns = pd.Series([6, -5.4], index=[\"Charles\", \"Chuck\"], name=\"heights\") \\ns \\nOut[7]:\\nIn [8]:\\n# Use a series in numpy functions. Here we take exponential of the series \\nnp.abs(s)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 2, 'page_label': '3'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 3/35\\nCharles    6.0 \\nChuck      5.4 \\nName: heights, dtype: float64\\nElementwise arithmetic operations on pandas Series are similar to numpy ndarray\\'s:\\nCharles    1006.0 \\nChuck      1994.6 \\nName: heights, dtype: float64\\nBroadcasting is similar to numpy. If you add a single number to Series, it is broadcasted\\nthroughout the series.\\nCharles    1006.0 \\nChuck       994.6 \\nName: heights, dtype: float64\\nThe same is true for all binary operations such as multiplication, division, subtraction or evern\\nconditionals operations\\nCharles    False \\nChuck       True \\nName: heights, dtype: bool\\nAccessing elements of Series (Indexing)\\n1. Each item in a Series object has a unique identifier called the *index label*. \\n2. By default, it is the rank of the item in the Series (starting at 0) but you can also set the\\nindex labels manually \\n3. You can also use the series as a dictionary with manually set indexing as well as integer\\nindex like regular list.\\n--------------------------------------------------------------------------- \\nNameError                                 Traceback (most recent call last) \\n<ipython-input-12-630081a5992e> in <module> \\n----> 1 s2 \\nNameError: name \\'s2\\' is not defined\\nUsing as a dictionary\\n--------------------------------------------------------------------------- \\nNameError                                 Traceback (most recent call last) \\n<ipython-input-13-05e1d0e8f3e2> in <module> \\nOut[8]:\\nIn [9]:\\ns + np.array([1000,2000]) \\nOut[9]:\\nIn [10]:\\ns + 1000 \\nOut[10]:\\nIn [11]:\\ns < 0 \\nOut[11]:\\nIn [12]:\\ns2 \\nIn [13]:\\ns2[\"Chuck\"]'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 3, 'page_label': '4'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 4/35\\n----> 1 s2[\"Chuck\"] \\nNameError: name \\'s2\\' is not defined\\nUsing as a list with integer index\\n--------------------------------------------------------------------------- \\nNameError                                 Traceback (most recent call last) \\n<ipython-input-14-8755b941c10d> in <module> \\n----> 1 s2[1] \\nNameError: name \\'s2\\' is not defined\\nTo make it clear when you are accessing by label or by integer location, it is recommended to\\nalways use the loc attribute when accessing by label, and the iloc attribute when accessing by\\ninteger location:\\n--------------------------------------------------------------------------- \\nNameError                                 Traceback (most recent call last) \\n<ipython-input-15-6a9a5ccf47c3> in <module> \\n----> 1 s2.loc[\"Chuck\"] \\nNameError: name \\'s2\\' is not defined\\n--------------------------------------------------------------------------- \\nNameError                                 Traceback (most recent call last) \\n<ipython-input-16-f5692f3bd256> in <module> \\n----> 1 s2.iloc[1] \\nNameError: name \\'s2\\' is not defined\\nSlicing a Series also slices the index labels. This can lead to unexpected values being accessed\\nwhen using the default numeric labels, so be careful!\\n--------------------------------------------------------------------------- \\nNameError                                 Traceback (most recent call last) \\n<ipython-input-17-7aab0de711b6> in <module> \\n----> 1 s3 = s2.iloc[1:3] \\n      2 print(s3) \\n      3 s3.loc[\"Newtown\"] \\nNameError: name \\'s2\\' is not defined\\n0    1000 \\n1    1001 \\n2    1002 \\n3    1003 \\ndtype: int64\\nIn [14]:\\ns2[1] \\nIn [15]:\\ns2.loc[\"Chuck\"] \\nIn [16]:\\ns2.iloc[1] \\nIn [17]:\\ns3 = s2.iloc[1:3] \\nprint(s3) \\ns3.loc[\"Newtown\"] \\nIn [18]:\\ns2 = pd.Series([1000, 1001, 1002, 1003]) \\ns2 \\nOut[18]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 4, 'page_label': '5'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 5/35\\n1    1001 \\n2    1002 \\n3    1003 \\ndtype: int64\\nThe first element has index label 1. The element with index label 0 is absent from the slice\\n--------------------------------------------------------------------------- \\nKeyError                                  Traceback (most recent call last) \\n<ipython-input-20-e4a7d3f28305> in <module> \\n----> 1 s2_slice[0] \\n~/anaconda3/lib/python3.8/site-packages/pandas/core/series.py in __getitem__(self, k\\ney) \\n    866         key = com.apply_if_callable(key, self) \\n    867         try: \\n--> 868             result = self.index.get_value(self, key) \\n    869  \\n    870             if not is_scalar(result): \\n~/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py in get_value(sel\\nf, series, key) \\n   4372         k = self._convert_scalar_indexer(k, kind=\\'getitem\\') \\n   4373         try: \\n-> 4374             return self._engine.get_value(s, k, \\n   4375                                           tz=getattr(series.dtype, \\'tz\\', Non\\ne)) \\n   4376         except KeyError as e1: \\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value() \\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_value() \\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc() \\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get\\n_item() \\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get\\n_item() \\nKeyError: 0\\nBut we can access elements by integer location using the iloc attribute.\\n1001\\nSo, we should always use loc and iloc to access Series objects.\\nAutomatic alignment\\nFor operations involving multiple series pandas automatically aligns items by matching the index\\nlabels.\\nIn [19]:\\ns2_slice = s2[1:] \\ns2_slice \\nOut[19]:\\nIn [20]:\\ns2_slice[0] \\nIn [21]:\\ns2_slice.iloc[0] \\nOut[21]:\\nIn [22]:\\ns2 = pd.Series([231, -23, -99, 100], index=[\"Newton\", \"Chuck\", \"Darwin\", \"Elijah\"])'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 5, 'page_label': '6'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 6/35\\nNewton    231 \\nChuck     -23 \\nDarwin    -99 \\nElijah    100 \\ndtype: int64 \\nCharles    122 \\nDarwin    -312 \\nChuck      123 \\nElijah     -20 \\ndtype: int64 \\nCharles      NaN \\nChuck      100.0 \\nDarwin    -411.0 \\nElijah      80.0 \\nNewton       NaN \\ndtype: float64\\nSince Charles is missing from s2 and Newton is missing from s3, these have a NaN result value.\\n(ie. Not-a-Number means missing).\\nAutomatic alignment is very handy when working with data that may come from various sources\\nwith varying structure and missing items. But if you forget to set the right index labels, you can\\nhave surprising results:\\ns2 = [231 -23 -99 100] \\ns5 = [ 100  -10  999 -999] \\n0        NaN \\n1        NaN \\n2        NaN \\n3        NaN \\nChuck    NaN \\nDarwin   NaN \\nElijah   NaN \\nNewton   NaN \\ndtype: float64\\nPandas could not align the Series, since their labels do not match at all, hence the full NaN\\nresult.\\nPlotting a Series\\nUsing matplotlib is easy with a Series.plot() call\\ns3 = pd.Series([122, -312, 123, -20], index=[\"Charles\", \"Darwin\", \"Chuck\", \"Elijah\"]\\nprint(s2) \\nprint(s3) \\n \\ns2 + s3 \\nOut[22]:\\nIn [23]:\\ns5 = pd.Series([100,-10,999,-999]) \\nprint(\"s2 =\", s2.values) \\nprint(\"s5 =\", s5.values) \\n \\ns2 + s5 \\nOut[23]:\\nIn [24]:\\n%matplotlib inline \\nimport matplotlib.pyplot as plt \\n \\npressure = [2,6.1,6.1,5.7,4.4,5.1,6.1,6,.5,.2,4.7,4.1,3.9,3.5] \\n \\ns7 = pd.Series(pressure, name=\"Pressure\")'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 6, 'page_label': '7'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 7/35\\nThere are a variety of plotting options in pandas and we will visiting those in the visualisation\\nsection.\\nDataFrame objects\\nA DataFrame object represents a spreadsheet, with cell values, column names and row index\\nlabels. We can define expressions to compute columns based on other columns, create pivot-\\ntables, group rows, draw graphs, etc.\\nYou can consider DataFrames as dictionaries of Series.\\nCreating a DataFrame\\nYou can create a DataFrame by passing a dictionary of Series objects:\\nweight birthyear children hobby\\nalice 68 1985 NaN Biking\\nbob 83 1984 3.0 Dancing\\ncharles 112 1992 0.0 NaN\\nA few things to note:\\nthe Series were automatically aligned based on their index,\\nmissing values are represented as NaN,\\nSeries names are ignored (the name \"year\" was dropped),\\nDataFrames are displayed nicely in Jupyter notebooks\\ns7.plot() \\nplt.show() \\nIn [25]:\\npeople_dict = { \\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]), \\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], na\\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]), \\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]), \\n} \\npeople = pd.DataFrame(people_dict) \\npeople \\nOut[25]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 7, 'page_label': '8'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 8/35\\nYou can access columns pretty much as you would expect. They are returned as Series\\nobjects:\\nalice      1985 \\nbob        1984 \\ncharles    1992 \\nName: birthyear, dtype: int64\\nYou can also get multiple columns at once:\\nbirthyear hobby\\nalice 1985 Biking\\nbob 1984 Dancing\\ncharles 1992 NaN\\nIf you pass a list of columns and/or index row labels to the DataFrame constructor, it will\\nguarantee that these columns and/or rows will exist, in that order, and no other column/row will\\nexist. For example:\\nbirthyear weight height\\nbob 1984.0 83.0 NaN\\nalice 1985.0 68.0 NaN\\neugene NaN NaN NaN\\nAnother convenient way to create a DataFrame is to pass all the values to the constructor as\\nan ndarray, or a list of lists, and specify the column names and row index labels separately:\\nbirthyear children hobbyweight\\nIn [26]:\\npeople[\"birthyear\"] \\nOut[26]:\\nIn [27]:\\npeople[[\"birthyear\", \"hobby\"]] \\nOut[27]:\\nIn [28]:\\nd2 = pd.DataFrame( \\n        people_dict, \\n        columns=[\"birthyear\", \"weight\", \"height\"], \\n        index=[\"bob\", \"alice\", \"eugene\"] \\n     ) \\nd2 \\nOut[28]:\\nIn [29]:\\nvalues = np.array([ \\n            [1985, np.nan, \"Biking\",   68], \\n            [1984, 3,      \"Dancing\",  83], \\n            [1992, 0,      np.nan,    112] \\n         ]) \\nd3 = pd.DataFrame( \\n        values, \\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"], \\n        index=[\"alice\", \"bob\", \"charles\"] \\n     ) \\nd3 \\nOut[29]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 8, 'page_label': '9'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pandas… 9/35\\nbirthyear children hobbyweight\\nalice 1985 nan Biking 68\\nbob 1984 3 Dancing 83\\ncharles 1992 0 nan 112\\nTo specify missing values, you can either use np.nan or NumPy\\'s masked arrays:\\nInstead of an ndarray, you can also pass a DataFrame object:\\nhobbychildren\\nalice Biking nan\\nbob Dancing 3\\nIt is also possible to create a DataFrame with a dictionary (or list) of dictionaries (or list):\\nbirthyear hobbyweight children\\nalice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\ncharles 1992 NaN 112 0.0\\nMulti-indexing\\nIf all columns are tuples of the same size, then they are understood as a multi-index. The same\\ngoes for row index labels. For example:\\nIn [30]:\\nd4 = pd.DataFrame( \\n         d3, \\n         columns=[\"hobby\", \"children\"], \\n         index=[\"alice\", \"bob\"] \\n     ) \\nd4 \\nOut[30]:\\nIn [31]:\\npeople = pd.DataFrame({ \\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992}, \\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"}, \\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112}, \\n    \"children\": {\"bob\": 3, \"charles\": 0} \\n}) \\npeople \\nOut[31]:\\nIn [32]:\\ndict1 = { \\n    (\"public\", \"birthyear\"): \\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\\n    (\"public\", \"hobby\"): \\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"}, \\n    (\"private\", \"weight\"): \\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112}, \\n    (\"private\", \"children\"): \\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0} \\n  }'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 9, 'page_label': '10'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 10/35\\n{(\\'public\\', \\'birthyear\\'): {(\\'Paris\\', \\'alice\\'): 1985, (\\'Paris\\', \\'bob\\'): 1984, (\\'Londo\\nn\\', \\'charles\\'): 1992}, (\\'public\\', \\'hobby\\'): {(\\'Paris\\', \\'alice\\'): \\'Biking\\', (\\'Paris\\', \\n\\'bob\\'): \\'Dancing\\'}, (\\'private\\', \\'weight\\'): {(\\'Paris\\', \\'alice\\'): 68, (\\'Paris\\', \\'bo\\nb\\'): 83, (\\'London\\', \\'charles\\'): 112}, (\\'private\\', \\'children\\'): {(\\'Paris\\', \\'alice\\'): \\nnan, (\\'Paris\\', \\'bob\\'): 3, (\\'London\\', \\'charles\\'): 0}} \\npublic private\\nbirthyear hobbyweight children\\nLondoncharles 1992 NaN 112 0.0\\nParis alice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\nYou can now get a DataFrame containing all the \"public\" columns very simply:\\nbirthyear hobby\\nLondoncharles 1992 NaN\\nParis alice 1985 Biking\\nbob 1984 Dancing\\nLondon  charles        NaN \\nParis   alice       Biking \\n        bob        Dancing \\nName: (public, hobby), dtype: object \\nLondon  charles        NaN \\nParis   alice       Biking \\n        bob        Dancing \\nName: hobby, dtype: object \\nDropping a level\\nLet\\'s look at d5 again:\\n \\nprint(dict1) \\nIn [46]:\\nd5 = pd.DataFrame( \\n  { \\n    (\"public\", \"birthyear\"): \\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\\n    (\"public\", \"hobby\"): \\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"}, \\n    (\"private\", \"weight\"): \\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112}, \\n    (\"private\", \"children\"): \\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0} \\n  } \\n) \\nd5 \\nOut[46]:\\nIn [47]:\\nd5[\"public\"] \\nOut[47]:\\nIn [48]:\\nprint(d5[\"public\", \"hobby\"]) \\n# Same result as  \\nprint(d5[\"public\"][\"hobby\"])'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 10, 'page_label': '11'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Panda… 11/35\\npublic private\\nbirthyear hobbyweight children\\nLondoncharles 1992 NaN 112 0.0\\nParis alice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\nThere are two levels of columns, and two levels of indices. We can drop a column level by calling\\ndroplevel() (the same goes for indices):\\npublic private\\nbirthyear hobbyweight children\\n(1992, nan, 112, 0.0)1992 NaN 112 0.0\\n(1985, Biking, 68, nan)1985 Biking 68 NaN\\n(1984, Dancing, 83, 3.0)1984 Dancing 83 3.0\\nTransposing\\nYou can swap columns and indices using the T attribute:\\nParis Paris London\\nbirthyear 1985 1984 1992\\nhobbyBikingDancing NaN\\nweight 68 83 112\\nchildren NaN 3 0\\n(1992, nan, 112, 0.0)(1985, Biking, 68, nan)(1984, Dancing, 83, 3.0)\\npublicbirthyear 1992 1985 1984\\nhobby NaN Biking Dancing\\nprivate weight 112 68 83\\nIn [44]:\\nd5 \\nOut[44]:\\nIn [49]:\\n#d5.columns = d5.columns.droplevel(level = 0) \\n \\n#d5.columns = d5.columns.droplevel(level = 0) \\nd5.index = d5.index.droplevel(level=10) \\nd5 \\nOut[49]:\\nIn [64]:\\nd6 = d5.T \\nd6 \\nOut[64]:\\nIn [51]:\\nd5.transpose() \\nOut[51]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 11, 'page_label': '12'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 12/35\\n(1992, nan, 112, 0.0)(1985, Biking, 68, nan)(1984, Dancing, 83, 3.0)\\nchildren 0 NaN 3\\nStacking and unstacking levels\\nCalling the stack() method will push the lowest column level after the lowest index:\\npublic private\\nbirthyear hobbyweight children\\nParis alice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\nLondoncharles 1992 NaN 112 0.0\\nprivate public\\nParis alice birthyear NaN 1985\\nhobby NaN Biking\\nweight 68.0 NaN\\nbob birthyear NaN 1984\\nchildren 3.0 NaN\\nhobby NaN Dancing\\nweight 83.0 NaN\\nLondoncharlesbirthyear NaN 1992\\nchildren 0.0 NaN\\nweight 112.0 NaN\\nNote that many NaN values appeared. This makes sense because many new combinations did\\nnot exist before (eg. there was no bob in London).\\nCalling unstack() will do the reverse, once again creating many NaN values.\\nprivate public\\nbirthyear children hobbyweight birthyear children hobbyweight\\nLondoncharles NaN 0.0 NaN 112.0 1992 NaN NaN NaN\\nIn [68]:\\nd6 = d5  \\nd6 \\nOut[68]:\\nIn [69]:\\nd7 = d6.stack() \\nd7 \\nOut[69]:\\nIn [70]:\\nd8 = d7.unstack() \\nd8 \\nOut[70]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 12, 'page_label': '13'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 13/35\\nprivate public\\nbirthyear children hobbyweight birthyear children hobbyweight\\nParis alice NaN NaN NaN 68.0 1985 NaN Biking NaN\\nbob NaN 3.0 NaN 83.0 1984 NaN Dancing NaN\\nIf we call unstack again, we end up with a Series object:\\nprivate  birthyear  alice    London        NaN \\n                             Paris         NaN \\n                    bob      London        NaN \\n                             Paris         NaN \\n                    charles  London        NaN \\n                             Paris         NaN \\n         children   alice    London        NaN \\n                             Paris         NaN \\n                    bob      London        NaN \\n                             Paris           3 \\n                    charles  London          0 \\n                             Paris         NaN \\n         hobby      alice    London        NaN \\n                             Paris         NaN \\n                    bob      London        NaN \\n                             Paris         NaN \\n                    charles  London        NaN \\n                             Paris         NaN \\n         weight     alice    London        NaN \\n                             Paris          68 \\n                    bob      London        NaN \\n                             Paris          83 \\n                    charles  London        112 \\n                             Paris         NaN \\npublic   birthyear  alice    London        NaN \\n                             Paris        1985 \\n                    bob      London        NaN \\n                             Paris        1984 \\n                    charles  London       1992 \\n                             Paris         NaN \\n         children   alice    London        NaN \\n                             Paris         NaN \\n                    bob      London        NaN \\n                             Paris         NaN \\n                    charles  London        NaN \\n                             Paris         NaN \\n         hobby      alice    London        NaN \\n                             Paris      Biking \\n                    bob      London        NaN \\n                             Paris     Dancing \\n                    charles  London        NaN \\n                             Paris         NaN \\n         weight     alice    London        NaN \\n                             Paris         NaN \\n                    bob      London        NaN \\n                             Paris         NaN \\n                    charles  London        NaN \\n                             Paris         NaN \\ndtype: object\\nIn [72]:\\nd9 = d8.unstack() \\ndummy = d9.unstack() \\ndummy \\nOut[72]:\\nIn [74]:\\nd9'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 13, 'page_label': '14'}, page_content=\"2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 14/35\\nThe stack() and unstack() methods let you select the level to stack/unstack. You can\\neven stack/unstack multiple levels at once:\\nalice bob charles\\nLondonprivate children NaN NaN 0\\nweight NaN NaN 112\\npublicbirthyear NaN NaN 1992\\nParis private children NaN 3 NaN\\nweight 68 83 NaN\\npublicbirthyear 1985 1984 NaN\\nhobbyBikingDancing NaN\\nMost methods return modified copies\\nAs you may have noticed, the stack() and unstack() methods do not modify the object\\nthey apply to. Instead, they work on a copy and return that copy. This is true of most methods in\\npandas.\\nAccessing rows\\nLet's go back to the people DataFrame:\\nbirthyear hobbyweight children\\nalice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\ncharles 1992 NaN 112 0.0\\nThe loc attribute lets you access rows instead of columns. The result is a Series object in\\nwhich the DataFrame's column names are mapped to row index labels:\\nprivate ...\\nbirthyear children hobbyweight ... birthyear\\nalice bob charlesalice bob charlesalice bob charles alice ... charlesalice\\nprivate ...\\nbirthyear children hobbyweight ... birthyear\\nalice bob charlesalice bob charlesalice bob charles alice ... charlesalice\\nLondonNaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN ... 1992 NaN\\nParis NaN NaN NaN NaN 3.0 NaN NaN NaN NaN 68.0 ... NaN NaN\\n2 rows × 24 columns\\nOut[74]:\\nIn [75]:\\nd10 = d9.stack(level = (0,1)) \\nd10 \\nOut[75]:\\nIn [76]:\\npeople \\nOut[76]:\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 14, 'page_label': '15'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 15/35\\nbirthyear    1992 \\nhobby         NaN \\nweight        112 \\nchildren        0 \\nName: charles, dtype: object\\nYou can also access rows by integer location using the iloc attribute:\\nbirthyear       1984 \\nhobby        Dancing \\nweight            83 \\nchildren           3 \\nName: bob, dtype: object\\nYou can also get a slice of rows, and this returns a DataFrame object:\\nbirthyear hobbyweight children\\nbob 1984 Dancing 83 3.0\\ncharles 1992 NaN 112 0.0\\nFinally, you can pass a boolean array to get the matching rows:\\nbirthyear hobbyweight children\\nalice 1985 Biking 68 NaN\\ncharles 1992 NaN 112 0.0\\nbirthyear hobbyweight children\\nalice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\ncharles 1992 NaN 112 0.0\\nalice       True \\nbob         True \\ncharles    False \\nName: birthyear, dtype: bool\\nThis is most useful when combined with boolean expressions:\\nIn [77]:\\npeople.loc[\"charles\"] \\nOut[77]:\\nIn [78]:\\npeople.iloc[1] \\nOut[78]:\\nIn [79]:\\npeople.iloc[1:3] \\nOut[79]:\\nIn [80]:\\npeople[np.array([True, False, True])] \\nOut[80]:\\nIn [81]:\\npeople \\nOut[81]:\\nIn [82]:\\npeople[\"birthyear\"] < 1990 \\nOut[82]:\\nIn [74]:\\npeople[people[\"birthyear\"] < 1990]'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 15, 'page_label': '16'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 16/35\\nbirthyear children hobbyweight\\nalice 1985 NaN Biking 68\\nbob 1984 3.0 Dancing 83\\nAdding and removing columns\\nYou can generally treat DataFrame objects like dictionaries of Series, so the following work\\nfine:\\nbirthyear hobbyweight children\\nalice 1985 Biking 68 NaN\\nbob 1984 Dancing 83 3.0\\ncharles 1992 NaN 112 0.0\\nhobbyweight age over 30\\nalice Biking 68 35 True\\nbob Dancing 83 36 True\\ncharles NaN 112 28 False\\nalice      1985 \\nbob        1984 \\ncharles    1992 \\nName: birthyear, dtype: int64\\nWhen you add a new colum, it must have the same number of rows. Missing rows are filled with\\nNaN, and extra rows are ignored:\\nhobbyweight age over 30 pets\\nOut[74]:\\nIn [106 …\\npeople = pd.DataFrame({ \\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992}, \\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"}, \\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112}, \\n    \"children\": {\"bob\": 3, \"charles\": 0} \\n}) \\npeople \\nOut[106 …\\nIn [107 …\\npeople[\"age\"] = 2020 - people[\"birthyear\"]  # adds a new column \"age\" \\npeople[\"over 30\"] = people[\"age\"] > 30      # adds another column \"over 30\" \\nbirthyears = people.pop(\"birthyear\")        # removes the columns \"birthyear\" \\ndel people[\"children\"] \\n \\npeople \\nOut[107 …\\nIn [108 …\\nbirthyears \\nOut[108 …\\nIn [109 …\\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice is missing\\npeople \\nOut[109 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 16, 'page_label': '17'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 17/35\\nhobbyweight age over 30 pets\\nalice Biking 68 35 True NaN\\nbob Dancing 83 36 True 0.0\\ncharles NaN 112 28 False 5.0\\nWhen adding a new column, it is added at the end (on the right) by default. You can also insert a\\ncolumn anywhere else using the insert() method:\\nhobbyheightweight age over 30 pets\\nalice Biking 172 68 35 True NaN\\nbob Dancing 181 83 36 True 0.0\\ncharles NaN 185 112 28 False 5.0\\nAssigning new columns\\nYou can also create new columns by calling the assign() method. Note that this returns a\\nnew DataFrame object, the original is not modified:\\nhobbyheightweight age over 30 pets body_mass_indexhas_petsheight_gt_25\\nalice Biking 172 68 35 True NaN 22.985398 False True\\nbob Dancing 181 83 36 True 0.0 25.335002 False True\\ncharles NaN 185 112 28 False 5.0 32.724617 True True\\nEvaluating an expression\\nA great feature supported by pandas is expression evaluation. This relies on the numexpr\\nlibrary which must be installed.\\nalice      False \\nbob         True \\ncharles     True \\ndtype: bool\\nAssignment expressions are also supported. Let\\'s set inplace=True to directly modify the \\nDataFrame rather than getting a modified copy:\\nIn [110 …\\npeople.insert(1, \"height\", [172, 181, 185]) \\npeople \\nOut[110 …\\nIn [116 …\\npeople.assign( \\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2, \\n    has_pets = people[\"pets\"] > 0, \\n    height_gt_25 = people[\\'height\\']>25 \\n) \\nOut[116 …\\nIn [117 …\\npeople.eval(\"weight / (height/100) ** 2 > 25\") \\nOut[117 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 17, 'page_label': '18'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 18/35\\nhobbyheightweight age over 30 pets body_mass_index\\nalice Biking 172 68 35 True NaN 22.985398\\nbob Dancing 181 83 36 True 0.0 25.335002\\ncharles NaN 185 112 28 False 5.0 32.724617\\nYou can use a local or global variable in an expression by prefixing it with \\'@\\':\\nhobbyheightweight age over 30 pets body_mass_indexoverweight\\nalice Biking 172 68 35 True NaN 22.985398 False\\nbob Dancing 181 83 36 True 0.0 25.335002 False\\ncharles NaN 185 112 28 False 5.0 32.724617 True\\nQuerying a DataFrame\\nThe query() method lets you filter a DataFrame based on a query expression:\\nhobbyheightweight age over 30 pets body_mass_indexoverweight\\nbob Dancing 181 83 34 True 0.0 25.335002 False\\nSorting a DataFrame\\nYou can sort a DataFrame by calling its sort_index method. By default it sorts the rows by\\ntheir index label, in ascending order, but let\\'s reverse the order:\\nhobbyheightweight age over 30 pets body_mass_indexoverweight\\nalice Biking 172 68 35 True NaN 22.985398 False\\nbob Dancing 181 83 36 True 0.0 25.335002 False\\ncharles NaN 185 112 28 False 5.0 32.724617 True\\nNote that sort_index returned a sorted c op y  of the DataFrame. To modify people directly,\\nwe can set the inplace argument to True. Also, we can sort the columns instead of the rows\\nby setting axis=1:\\nIn [119 …\\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True) \\npeople \\nOut[119 …\\nIn [120 …\\noverweight_threshold = 30 \\npeople.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True) \\npeople \\nOut[120 …\\nIn [88]:\\npeople.query(\"age > 30 and pets == 0\") \\nOut[88]:\\nIn [122 …\\npeople.sort_index(ascending=True) \\nOut[122 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 18, 'page_label': '19'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 19/35\\nage body_mass_indexheight hobbyover 30 overweight pets weight\\nalice 35 22.985398 172 Biking True False NaN 68\\nbob 36 25.335002 181 Dancing True False 0.0 83\\ncharles 28 32.724617 185 NaN False True 5.0 112\\nTo sort the DataFrame by the values instead of the labels, we can use sort_values and\\nspecify the column to sort by:\\nage body_mass_indexheight hobbyover 30 overweight pets weight\\ncharles 28 32.724617 185 NaN False True 5.0 112\\nalice 35 22.985398 172 Biking True False NaN 68\\nbob 36 25.335002 181 Dancing True False 0.0 83\\nPlotting a DataFrame\\nJust like for Series, pandas makes it easy to draw nice graphs based on a DataFrame.\\nFor example, it is trivial to create a line plot from a DataFrame\\'s data by calling its plot\\nmethod:\\n<matplotlib.axes._subplots.AxesSubplot at 0x11f6cda90>\\nYou can pass extra arguments supported by matplotlib\\'s functions. For example, we can create\\nscatterplot and pass it a list of sizes using the s argument of matplotlib\\'s scatter()\\nfunction:\\nIn [123 …\\npeople.sort_index(axis=1, inplace=True) \\npeople \\nOut[123 …\\nIn [126 …\\npeople.sort_values(by=[\"age\",\"height\"], inplace=True) \\npeople \\nOut[126 …\\nIn [129 …\\npeople.plot(kind = \"scatter\", x = \"body_mass_index\", y = \"height\") \\n#plt.show() \\nOut[129 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 19, 'page_label': '20'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 20/35\\n<matplotlib.axes._subplots.AxesSubplot at 0x11f85ec70>\\nAgain, there are way too many options to list here: the best option is to scroll through the\\nVisualization page in pandas\\' documentation, find the plot you are interested in and look at the\\nexample code.\\nOperations on DataFrames\\nAlthough DataFrames do not try to mimick NumPy arrays, there are a few similarities. Let\\'s\\ncreate a DataFrame to demonstrate this:\\nsep oct nov\\nalice 8 8 9\\nbob 10 9 9\\ncharles 4 8 2\\ndarwin 9 10 10\\nYou can apply NumPy mathematical functions on a DataFrame: the function is applied to all\\nvalues:\\nsep oct nov\\nalice 2.8284272.8284273.000000\\nbob 3.1622783.0000003.000000\\ncharles2.0000002.8284271.414214\\ndarwin 3.0000003.1622783.162278\\nIn [131 …\\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200]) \\n#plt.show() \\nOut[131 …\\nIn [132 …\\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) \\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"b\\ngrades \\nOut[132 …\\nIn [133 …\\nnp.sqrt(grades) \\nOut[133 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 20, 'page_label': '21'}, page_content=\"2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 21/35\\nSimilarly, adding a single value to a DataFrame will add that value to all elements in the \\nDataFrame. This is called br o adcasting :\\nsep oct nov\\nalice 9 9 10\\nbob 11 10 10\\ncharles 5 9 3\\ndarwin 10 11 11\\nOf course, the same is true for all other binary operations, including arithmetic (*, /, **...)\\nand conditional (>, ==...) operations:\\nsep oct nov\\nalice True True True\\nbob True True True\\ncharlesFalse True False\\ndarwin True True True\\nAggregation operations, such as computing the max, the sum or the mean of a DataFrame,\\napply to each column, and you get back a Series object:\\nsep    7.75 \\noct    8.75 \\nnov    7.50 \\ndtype: float64\\nsep oct nov\\nalice 8 8 9\\nbob 10 9 9\\ncharles 4 8 2\\ndarwin 9 10 10\\nThe all method is also an aggregation operation: it checks whether all values are True or\\nnot. Let's see during which months all students got a grade greater than 5:\\nIn [134 …\\ngrades + 1 \\nOut[134 …\\nIn [137 …\\ngrades >= 5 \\nOut[137 …\\nIn [138 …\\ngrades.mean() \\nOut[138 …\\nIn [139 …\\ngrades \\nOut[139 …\\nIn [99]:\\n(grades > 5).all()\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 21, 'page_label': '22'}, page_content=\"2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 22/35\\nsep    False \\noct     True \\nnov    False \\ndtype: bool\\nMost of these functions take an optional axis parameter which lets you specify along which\\naxis of the DataFrame you want the operation executed. The default is axis=0, meaning that\\nthe operation is executed vertically (on each column). You can set axis=1 to execute the\\noperation horizontally (on each row). For example, let's find out which students had all grades\\ngreater than 5:\\nalice       True \\nbob         True \\ncharles    False \\ndarwin      True \\ndtype: bool\\nThe any method returns True if any value is True. Let's see who got at least one grade 10:\\nalice      False \\nbob         True \\ncharles    False \\ndarwin      True \\ndtype: bool\\nIf you add a Series object to a DataFrame (or execute any other binary operation), pandas\\nattempts to broadcast the operation to all r o ws  in the DataFrame. This only works if the \\nSeries has the same size as the DataFrames rows. For example, let's substract the mean of\\nthe DataFrame (a Series object) from the DataFrame:\\nsep oct nov\\nalice 8 8 9\\nbob 10 9 9\\ncharles 4 8 2\\ndarwin 9 10 10\\nsep    7.75 \\noct    8.75 \\nnov    7.50 \\ndtype: float64\\nsep oct nov\\nalice 0.25 -0.75 1.5\\nOut[99]:\\nIn [100 …\\n(grades > 5).all(axis = 1) \\nOut[100 …\\nIn [140 …\\n(grades == 10).any(axis = 1) \\nOut[140 …\\nIn [141 …\\ngrades \\nOut[141 …\\nIn [142 …\\ngrades.mean() \\nOut[142 …\\nIn [102 …\\ngrades - grades.mean()  # equivalent to: grades - [7.75, 8.75, 7.50] \\nOut[102 …\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 22, 'page_label': '23'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 23/35\\nsep oct nov\\nbob 2.25 0.25 1.5\\ncharles-3.75 -0.75 -5.5\\ndarwin 1.25 1.25 2.5\\nIf you want to substract the global mean from every grade, here is one way to do it:\\nsep oct nov\\nalice 0.0 0.0 1.0\\nbob 2.0 1.0 1.0\\ncharles-4.0 0.0 -6.0\\ndarwin 1.0 2.0 2.0\\nAutomatic alignment\\nSimilar to Series, when operating on multiple DataFrames, pandas automatically aligns\\nthem by row index label, but also by column names. Let\\'s create a DataFrame with bonus\\npoints for each person from October to December:\\nsep oct nov\\nalice 8 8 9\\nbob 10 9 9\\ncharles 4 8 2\\ndarwin 9 10 10\\noct nov dec\\nbob 0.0 NaN 2.0\\ncolin NaN 1.0 0.0\\ndarwin 0.0 1.0 0.0\\ncharles 3.0 3.0 0.0\\nsep oct nov\\nIn [104 …\\ngrades - grades.values.mean() # substracts the global mean (8.00) from all grades \\nOut[104 …\\nIn [149 …\\ngrades \\nOut[149 …\\nIn [144 …\\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) \\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\"\\nbonus_points \\nOut[144 …\\nIn [145 …\\ngrades \\nOut[145 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 23, 'page_label': '24'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 24/35\\nsep oct nov\\nalice 8 8 9\\nbob 10 9 9\\ncharles 4 8 2\\ndarwin 9 10 10\\ndec nov oct sep\\nalice NaN NaN NaN NaN\\nbob NaN NaN 9.0 NaN\\ncharlesNaN 5.0 11.0 NaN\\ncolin NaN NaN NaN NaN\\ndarwin NaN 11.0 10.0 NaN\\nAggregating with groupby\\nSimilar to the SQL language, you can group pandas dataframe into by different columns and\\nperform computations.\\nLets take the grades dataframe and introduce a column with NaN values to understand their\\nhandling\\nsep oct nov\\nalice 8 8 9\\nbob 10 9 9\\ncharles 4 8 2\\ndarwin 9 10 10\\nsep oct nov hobby\\nalice 8 8 9 Football\\nbob 10 9 9 NaN\\ncharles 4 8 2 Dancing\\ndarwin 9 10 10 Football\\nIn [146 …\\ngrades + bonus_points \\nOut[146 …\\nIn [25]:\\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]]) \\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"b\\ngrades \\nOut[25]:\\nIn [26]:\\ngrades[\"hobby\"] = [\"Football\", np.nan, \"Dancing\", \"Football\"] \\ngrades \\nOut[26]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 24, 'page_label': '25'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 25/35\\nNow let\\'s group data in this DataFrame by hobby:\\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x11c74a730>\\nNow lets compute some statistics on the data\\nsep oct nov\\nhobby\\nDancing4.0 8.0 2.0\\nFootball 8.5 9.0 9.5\\nsep oct nov\\nhobby\\nDancing4.0 8.0 2.0\\nFootball 8.5 9.0 9.5\\nNote that the NaN values have been skipped when computing the statistics.\\nPivot tables\\nSimilar to spreadsheets, you can create pivot tables in pandas for quick data summarization.\\nLet\\'s create a simple DataFrame to understand this:\\noct nov dec\\nbob 0.0 NaN 2.0\\ncolin NaN 1.0 0.0\\ndarwin 0.0 1.0 0.0\\ncharles 3.0 3.0 0.0\\nlevel_0 level_1 0\\n0 bob oct 0.0\\nIn [31]:\\ngrouped_grades = grades.groupby([\"hobby\"]) \\ngrouped_grades \\nOut[31]:\\nIn [32]:\\ngrouped_grades.mean() \\nOut[32]:\\nIn [36]:\\ngrouped_grades.median() \\nOut[36]:\\nIn [37]:\\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]]) \\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\"\\nbonus_points \\nOut[37]:\\nIn [38]:\\nmore_grades = bonus_points.stack().reset_index() \\nmore_grades \\nOut[38]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 25, 'page_label': '26'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 26/35\\nlevel_0 level_1 0\\n1 bob dec 2.0\\n2 colin nov 1.0\\n3 colin dec 0.0\\n4 darwin oct 0.0\\n5 darwin nov 1.0\\n6 darwin dec 0.0\\n7 charles oct 3.0\\n8 charles nov 3.0\\n9 charles dec 0.0\\nnamemonthgradebonus\\n0 bob oct 0.0 NaN\\n1 bob dec 2.0 NaN\\n2 colin nov 1.0 NaN\\n3 colin dec 0.0 0.0\\n4 darwin oct 0.0 NaN\\n5 darwin nov 1.0 1.0\\n6 darwin dec 0.0 3.0\\n7 charles oct 3.0 3.0\\n8 charles nov 3.0 0.0\\n9 charles dec 0.0 1.0\\nNow we can use the pd.pivot_table() function to group by the name column. By default, \\npivot_table() computes the mean of each numeric column:\\nbonus grade\\nname\\nbob NaN 1.000000\\ncharles1.3333332.000000\\ncolin 0.0000000.500000\\nIn [39]:\\nmore_grades = bonus_points.stack().reset_index() \\n \\nmore_grades.columns = [\"name\", \"month\", \"grade\"] \\n \\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 1, 3, 3, 0, 1] \\n \\nmore_grades \\nOut[39]:\\nIn [40]:\\npd.pivot_table(more_grades, index=\"name\") \\nOut[40]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 26, 'page_label': '27'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 27/35\\nbonus grade\\nname\\ndarwin 2.0000000.333333\\nWe can change the aggregation function by setting the aggfunc argument, and we can also\\nspecify the list of columns whose values will be aggregated:\\nbonusgrade\\nname\\nbob NaN 2.0\\ncharles 3.0 3.0\\ncolin 0.0 1.0\\ndarwin 3.0 1.0\\nWe can also specify the columns to aggregate over horizontally, and request the grand totals\\nfor each row and column by setting margins=True:\\nmonthdec nov oct All\\nname\\nbob 2.0 NaN 0.0 1.000000\\ncharles 0.0 3.000000 3.0 2.000000\\ncolin 0.0 1.000000NaN 0.500000\\ndarwin 0.0 1.000000 0.0 0.333333\\nAll 0.5 1.666667 1.0 1.000000\\nFinally, we can specify multiple index or column names, and pandas will create multi-level\\nindices:\\nbonusgrade\\nnamemonth\\nalice nov NaN 9.00\\noct NaN 8.00\\nsep NaN 8.00\\nbob nov 2.000 10.00\\noct NaN 9.00\\nsep 0.000 10.00\\nIn [41]:\\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max) \\nOut[41]:\\nIn [42]:\\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=T\\nOut[42]:\\nIn [124 …\\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True) \\nOut[124 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 27, 'page_label': '28'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 28/35\\nbonusgrade\\nnamemonth\\ncharles nov 0.000 5.00\\noct 3.000 11.00\\nsep 3.000 4.00\\ndarwin nov 0.000 11.00\\noct 1.000 10.00\\nsep 0.000 9.00\\nAll 1.125 8.75\\nSaving & loading\\nPandas can save DataFrames to various backends, including file formats such as CSV, Excel,\\nJSON, HTML and HDF5, or to a SQL database. We will see how to save in csv, html and json\\nformat.\\nLet\\'s start by creating a DataFrame:\\nhobbyweight birthyear children\\nalice Biking 68.5 1985 NaN\\nbob Dancing 83.1 1984 3.0\\nSaving\\nLet\\'s save it to CSV, HTML and JSON:\\nDone! Let\\'s take a peek at what was saved:\\n# my_df.csv \\n,hobby,weight,birthyear,children \\nIn [52]:\\nmy_df = pd.DataFrame( \\n    [[\"Biking\", 68.5, 1985, np.nan], [\"Dancing\", 83.1, 1984, 3]],  \\n    columns=[\"hobby\",\"weight\",\"birthyear\",\"children\"], \\n    index=[\"alice\", \"bob\"] \\n) \\nmy_df \\nOut[52]:\\nIn [53]:\\nmy_df.to_csv(\"my_df.csv\") \\nmy_df.to_html(\"my_df.html\") \\nmy_df.to_json(\"my_df.json\") \\nIn [54]:\\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"): \\n    print(\"#\", filename) \\n    with open(filename, \"rt\") as f: \\n        print(f.read()) \\n        print()'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 28, 'page_label': '29'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 29/35\\nalice,Biking,68.5,1985, \\nbob,Dancing,83.1,1984,3.0 \\n# my_df.html \\n<table border=\"1\" class=\"dataframe\"> \\n  <thead> \\n    <tr style=\"text-align: right;\"> \\n      <th></th> \\n      <th>hobby</th> \\n      <th>weight</th> \\n      <th>birthyear</th> \\n      <th>children</th> \\n    </tr> \\n  </thead> \\n  <tbody> \\n    <tr> \\n      <th>alice</th> \\n      <td>Biking</td> \\n      <td>68.5</td> \\n      <td>1985</td> \\n      <td>NaN</td> \\n    </tr> \\n    <tr> \\n      <th>bob</th> \\n      <td>Dancing</td> \\n      <td>83.1</td> \\n      <td>1984</td> \\n      <td>3.0</td> \\n    </tr> \\n  </tbody> \\n</table> \\n# my_df.json \\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birt\\nhyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}} \\nNote that the index is saved as the first column (with no name) in a CSV file, as <th> tags in\\nHTML and as keys in JSON.\\nSaving to other formats works very similarly, but some formats require extra libraries to be\\ninstalled. For example, saving to Excel requires the openpyxl library:\\nRequirement already satisfied: openpyxl in /Users/himanshusharma/anaconda3/lib/pytho\\nn3.8/site-packages (3.0.4) \\nRequirement already satisfied: jdcal in /Users/himanshusharma/anaconda3/lib/python3.\\n8/site-packages (from openpyxl) (1.4.1) \\nRequirement already satisfied: et-xmlfile in /Users/himanshusharma/anaconda3/lib/pyt\\nhon3.8/site-packages (from openpyxl) (1.0.1) \\nLoading\\nNow let\\'s load our CSV file back into a DataFrame:\\nIn [46]:\\n!pip install openpyxl \\ntry: \\n    my_df.to_excel(\"my_df.xlsx\", sheet_name=\\'People\\') \\nexcept ImportError as e: \\n    print(e) \\nIn [55]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 29, 'page_label': '30'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 30/35\\nhobbyweight birthyear children\\nalice Biking 68.5 1985 NaN\\nbob Dancing 83.1 1984 3.0\\nCombining DataFrames\\nSQL-like joins\\nOne powerful feature of pandas is it\\'s ability to perform SQL-like joins on DataFrames. Various\\ntypes of joins are supported: inner joins, left/right outer joins and full joins. Let\\'s start by\\ncreating a couple of simple DataFrames:\\nstate city lat lng\\n0 CA San Francisco37.781334-122.416728\\n1 NY New York 40.705649-74.008344\\n2 FL Miami25.791100-80.320733\\n3 OH Cleveland41.473508-81.739791\\n4 UT Salt Lake City40.755851-111.896657\\npopulation city state\\n3 808976San FranciscoCalifornia\\n4 8363710 New York New-York\\n5 413201 Miami Florida\\n6 2242193 Houston Texas\\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0) \\nmy_df_loaded \\nOut[55]:\\nIn [4]:\\ncity_loc = pd.DataFrame( \\n    [ \\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728], \\n        [\"NY\", \"New York\", 40.705649, -74.008344], \\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791], \\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657] \\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\\ncity_loc \\nOut[4]:\\nIn [5]:\\ncity_pop = pd.DataFrame( \\n    [ \\n        [808976, \"San Francisco\", \"California\"], \\n        [8363710, \"New York\", \"New-York\"], \\n        [413201, \"Miami\", \"Florida\"], \\n        [2242193, \"Houston\", \"Texas\"] \\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"]) \\ncity_pop \\nOut[5]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 30, 'page_label': '31'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 31/35\\nNow let\\'s join these DataFrames using the merge() function:\\nstate_x city lat lng population state_y\\n0 CA San Francisco37.781334-122.416728 808976California\\n1 NY New York 40.705649-74.008344 8363710New-York\\n2 FL Miami25.791100-80.320733 413201 Florida\\nNote that both DataFrames have a column named state, so in the result they got renamed\\nto state_x and state_y.\\nAlso, note that Cleveland, Salt Lake City and Houston were dropped because they don\\'t exist in\\nboth  DataFrames. This is the equivalent of a SQL INNER JOIN. If you want a FULL OUTER \\nJOIN, where no city gets dropped and NaN values are added, you must specify \\nhow=\"outer\":\\nstate_x city lat lng population state_y\\n0 CA San Francisco37.781334-122.416728 808976.0California\\n1 NY New York 40.705649-74.0083448363710.0New-York\\n2 FL Miami25.791100-80.320733 413201.0 Florida\\n3 OH Cleveland41.473508-81.739791 NaN NaN\\n4 UT Salt Lake City40.755851-111.896657 NaN NaN\\n5 NaN Houston NaN NaN 2242193.0 Texas\\nOf course LEFT OUTER JOIN is also available by setting how=\"left\": only the cities present\\nin the left DataFrame end up in the result. Similarly, with how=\"right\" only cities in the right\\nDataFrame appear in the result. For example:\\nstate_x city lat lng population state_y\\n0 CA San Francisco37.781334-122.416728 808976California\\n1 NY New York 40.705649-74.008344 8363710New-York\\n2 FL Miami25.791100-80.320733 413201 Florida\\n3 NaN Houston NaN NaN 2242193 Texas\\nIf the key column names differ, you must use left_on and right_on. For example:\\nIn [6]:\\npd.merge(left=city_loc, right=city_pop, on=\"city\") \\nOut[6]:\\nIn [7]:\\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\") \\nall_cities \\nOut[7]:\\nIn [8]:\\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\") \\nOut[8]:\\nIn [9]:\\ncity_pop2 = city_pop.copy() \\ncity_pop2.columns = [\"population\", \"name\", \"state\"] \\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 31, 'page_label': '32'}, page_content=\"2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 32/35\\nstate_x city lat lng population name state_y\\n0 CA San Francisco37.781334-122.416728 808976San FranciscoCalifornia\\n1 NY New York 40.705649-74.008344 8363710 New York New-York\\n2 FL Miami25.791100-80.320733 413201 Miami Florida\\nConcatenation\\nRather than joining DataFrames, we may just want to concatenate them. That's what \\nconcat() is for:\\n<ipython-input-10-7a9b618ffc40>:1: FutureWarning: Sorting because non-concatenation \\naxis is not aligned. A future version \\nof pandas will change to not sort by default. \\nTo accept the future behavior, pass 'sort=False'. \\nTo retain the current behavior and silence the warning, pass 'sort=True'. \\n  result_concat = pd.concat([city_loc, city_pop]) \\ncity lat lng population state\\n0 San Francisco37.781334-122.416728 NaN CA\\n1 New York 40.705649-74.008344 NaN NY\\n2 Miami25.791100-80.320733 NaN FL\\n3 Cleveland41.473508-81.739791 NaN OH\\n4 Salt Lake City40.755851-111.896657 NaN UT\\n3 San Francisco NaN NaN 808976.0California\\n4 New York NaN NaN 8363710.0New-York\\n5 Miami NaN NaN 413201.0 Florida\\n6 Houston NaN NaN 2242193.0 Texas\\nNote that this operation aligned the data horizontally (by columns) but not vertically (by rows).\\nIn this example, we end up with multiple rows having the same index (eg. 3). Pandas handles this\\nrather gracefully:\\ncity lat lng population state\\n3 Cleveland41.473508-81.739791 NaN OH\\n3 San Francisco NaN NaN 808976.0California\\nOr you can tell pandas to just ignore the index:\\nOut[9]:\\nIn [10]:\\nresult_concat = pd.concat([city_loc, city_pop])\\nresult_concat \\nOut[10]:\\nIn [11]:\\nresult_concat.loc[3] \\nOut[11]:\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 32, 'page_label': '33'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 33/35\\n<ipython-input-12-ab7178b112b0>:1: FutureWarning: Sorting because non-concatenation \\naxis is not aligned. A future version \\nof pandas will change to not sort by default. \\nTo accept the future behavior, pass \\'sort=False\\'. \\nTo retain the current behavior and silence the warning, pass \\'sort=True\\'. \\n  pd.concat([city_loc, city_pop], ignore_index=True) \\ncity lat lng population state\\n0 San Francisco37.781334-122.416728 NaN CA\\n1 New York 40.705649-74.008344 NaN NY\\n2 Miami25.791100-80.320733 NaN FL\\n3 Cleveland41.473508-81.739791 NaN OH\\n4 Salt Lake City40.755851-111.896657 NaN UT\\n5 San Francisco NaN NaN 808976.0California\\n6 New York NaN NaN 8363710.0New-York\\n7 Miami NaN NaN 413201.0 Florida\\n8 Houston NaN NaN 2242193.0 Texas\\nNotice that when a column does not exist in a DataFrame, it acts as if it was filled with NaN\\nvalues. If we set join=\"inner\", then only columns that exist in both  DataFrames are\\nreturned:\\nstate city\\n0 CA San Francisco\\n1 NY New York\\n2 FL Miami\\n3 OH Cleveland\\n4 UT Salt Lake City\\n3 CaliforniaSan Francisco\\n4 New-York New York\\n5 Florida Miami\\n6 Texas Houston\\nYou can concatenate DataFrames horizontally instead of vertically by setting axis=1:\\nstate city lat lng population city state\\n0 CA San Francisco37.781334-122.416728 NaN NaN NaN\\nIn [12]:\\npd.concat([city_loc, city_pop], ignore_index=True) \\nOut[12]:\\nIn [145 …\\npd.concat([city_loc, city_pop], join=\"inner\") \\nOut[145 …\\nIn [146 …\\npd.concat([city_loc, city_pop], axis=1) \\nOut[146 …'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 33, 'page_label': '34'}, page_content='2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 34/35\\nstate city lat lng population city state\\n1 NY New York 40.705649-74.008344 NaN NaN NaN\\n2 FL Miami25.791100-80.320733 NaN NaN NaN\\n3 OH Cleveland41.473508-81.739791 808976.0San FranciscoCalifornia\\n4 UT Salt Lake City40.755851-111.8966578363710.0 New York New-York\\n5 NaN NaN NaN NaN 413201.0 Miami Florida\\n6 NaN NaN NaN NaN 2242193.0 Houston Texas\\nIn this case it really does not make much sense because the indices do not align well (eg.\\nCleveland and San Francisco end up on the same row, because they shared the index label 3).\\nSo let\\'s reindex the DataFrames by city name before concatenating:\\n<ipython-input-14-48a737537456>:1: FutureWarning: Sorting because non-concatenation \\naxis is not aligned. A future version \\nof pandas will change to not sort by default. \\nTo accept the future behavior, pass \\'sort=False\\'. \\nTo retain the current behavior and silence the warning, pass \\'sort=True\\'. \\n  pd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1) \\nstate lat lng population state\\nCleveland OH 41.473508-81.739791 NaN NaN\\nHouston NaN NaN NaN 2242193.0 Texas\\nMiami FL 25.791100-80.320733 413201.0 Florida\\nNew York NY 40.705649-74.0083448363710.0New-York\\nSalt Lake City UT 40.755851-111.896657 NaN NaN\\nSan FranciscoCA 37.781334-122.416728 808976.0California\\nThis looks a lot like a FULL OUTER JOIN, except that the state columns were not renamed\\nto state_x and state_y, and the city column is now the index.\\nThe append() method is a useful shorthand for concatenating DataFrames vertically:\\n/Users/himanshusharma/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:669\\n0: FutureWarning: Sorting because non-concatenation axis is not aligned. A future ve\\nrsion \\nof pandas will change to not sort by default. \\nTo accept the future behavior, pass \\'sort=False\\'. \\nTo retain the current behavior and silence the warning, pass \\'sort=True\\'. \\n  return concat(to_concat, ignore_index=ignore_index, \\ncity lat lng population state\\n0 San Francisco37.781334-122.416728 NaN CA\\nIn [14]:\\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1) \\nOut[14]:\\nIn [15]:\\ncity_loc.append(city_pop) \\nOut[15]:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m96', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36', 'creationdate': '2022-02-12T16:37:04+00:00', 'moddate': '2022-02-12T16:37:04+00:00', 'source': 'Pandas_PDF.pdf', 'total_pages': 35, 'page': 34, 'page_label': '35'}, page_content=\"2/12/22, 10:07 PM Pandas\\nfile:///C:/Users/Anilchoudary R/Desktop/SKILLO_VILLA/Course_Videos/Module_3/Pandas and statistic notebook/Pandas_and_Statistics/Pand… 35/35\\ncity lat lng population state\\n1 New York 40.705649-74.008344 NaN NY\\n2 Miami25.791100-80.320733 NaN FL\\n3 Cleveland41.473508-81.739791 NaN OH\\n4 Salt Lake City40.755851-111.896657 NaN UT\\n3 San Francisco NaN NaN 808976.0California\\n4 New York NaN NaN 8363710.0New-York\\n5 Miami NaN NaN 413201.0 Florida\\n6 Houston NaN NaN 2242193.0 Texas\\nAs always in pandas, the append() method does not  actually modify city_loc: it works on a\\ncopy and returns the modified copy.\\n'\\\\ndescribe()\\\\nhead()\\\\ntail()\\\\ninfo()\\\\n'\\nWhat next?\\nAs you probably noticed by now, pandas is quite a large library with man y  features. Although we\\nwent through the most important features, there is still a lot to discover. Probably the best way\\nto learn more is to get your hands dirty with some real-life data. It is also a good idea to go\\nthrough pandas' excellent documentation, in particular the Cookbook.\\nIn [13]:\\n#Some functions to be kept on the tips \\n \\n''' \\ndescribe() \\nhead() \\ntail() \\ninfo() \\n''' \\nOut[13]:\\nIn [ ]:\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read a pdf file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('Pandas_PDF.pdf')\n",
    "pdfDoc = loader.load()\n",
    "pdfDoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b19cf0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/integrations/document_loaders/sitemap/', 'title': 'Sitemap | 🦜️🔗 LangChain', 'description': 'Extends from the WebBaseLoader, SitemapLoader loads a sitemap from a given URL, and then scrapes and loads all pages in the sitemap, returning each page as a Document.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nSitemap | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentWe are growing and hiring for multiple roles for LangChain, LangGraph and LangSmith.  Join our team!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchProvidersAnthropicAWSGoogleHugging FaceMicrosoftOpenAIMoreProvidersAbsoAcreomActiveloop Deep LakeADS4GPTsAerospikeAgentQLAI21 LabsAimAINetworkAirbyteAirtableAlchemyAleph AlphaAlibaba CloudAnalyticDBAnnoyAnthropicAnyscaleApache Software FoundationApache DorisApifyAppleArangoDBArceeArcGISArgillaArizeArthurArxivAscendAskNewsAssemblyAIAstra DBAtlasAwaDBAWSAZLyricsAzure AIBAAIBagelBagelDBBaichuanBaiduBananaBasetenBeamBeautiful SoupBibTeXBiliBiliBittensorBlackboardbookend.aiBoxBrave SearchBreebs (Open Knowledge)Bright DataBrowserbaseBrowserlessByteDanceCassandraCerebrasCerebriumAIChaindeskChromaClarifaiClearMLClickHouseClickUpCloudflareClovaCnosDBCogneeCogniSwitchCohereCollege ConfidentialCometConfident AIConfluenceConneryContextContextual AICouchbaseCozeCrateDBC TransformersCTranslate2CubeDappierDashVectorDatabricksDatadog TracingDatadog LogsDataForSEODataheraldDedocDeepInfraDeeplakeDeepSeekDeepSparseDellDiffbotDingoDBDiscordDiscord (community loader)DocArrayDoclingDoctranDocugamiDocusaurusDriaDropboxDuckDBDuckDuckGo SearchE2BEden AIElasticsearchElevenLabsEmbedchainEpsillaEtherscanEverly AIEverNoteExaFacebook - MetaFalkorDBFaunaFeatherless AIFiddlerFigmaFireCrawlFireworks AIFlyteFMP Data (Financial Data Prep)Forefront AIFriendli AISmabblerGelGeopandasGitGitBookGitHubGitLabGOATGoldenGoodfireGoogleSerper - Google Search APIGooseAIGPT4AllGradientGraph RAGGraphsignalGrobidGroqGutenbergHacker NewsHazy ResearchHeliconeHologresHTML to textHuaweiHugging FaceHyperbrowserIBMIEIT SystemsiFixitiFlytekIMSDbInfinispan VSInfinityInfinoIntelIuguJaguarJavelin AI GatewayJenkinsJina AIJohnsnowlabsJoplinKDB.AIKineticaKoboldAIKonkoKoNLPYKùzuLabel StudiolakeFSLanceDBLangChain Decorators ✨LangFair: Use-Case Level LLM Bias and Fairness AssessmentsLangfuse 🪢LanternLindormLinkupLiteLLMLlamaIndexLlama.cppLlamaEdgellamafileLLMonitorLocalAILog10MariaDBMariTalkMarqoMediaWikiDumpMeilisearchMemcachedMemgraphMetalMicrosoftMilvusMindsDBMinimaxMistralAIMLflow AI Gateway for LLMsMLflowMLXModalModelScopeModern TreasuryMomentoMongoDBMongoDB AtlasMotherduckMotörheadMyScaleNAVERNeo4jNetmindNimbleNLPCloudNomicNotion DBNucliaNVIDIAObsidianOceanBaseOracle Cloud Infrastructure (OCI)OctoAIOllamaOntotext GraphDBOpenAIOpenGradientOpenLLMOpenSearchOpenWeatherMapOracleAI Vector SearchOutlineOutlinesOxylabsPandasPaymanAIPebbloPermitPerplexityPetalsPostgres EmbeddingPGVectorPineconePipelineAIPipeshiftPortkeyPredibasePrediction GuardPremAISWI-PrologPromptLayerPsychicPubMedPullMd LoaderPygmalionAIPyMuPDF4LLMQdrantRAGatouillerank_bm25Ray ServeRebuffRedditRedisRemembrallReplicateRoamSema4 (fka Robocorp)RocksetRunhouseRunpodRWKV-4SalesforceSambaNovaSAPScrapeGraph AISearchApiSearxNG Search APISemaDBSerpAPIShale ProtocolSingleStore Integrationscikit-learnSlackSnowflakespaCySparkSparkLLMSpreedlySQLiteStack ExchangeStarRocksStochasticAIStreamlitStripeSupabase (Postgres)NebulaTableauTaigaTairTavilyTelegramTencentTensorFlow DatasetsTiDBTigerGraphTigrisTiloresTogether AI2MarkdownTranswarpTrelloTrubricsTruLensTwitterTypesenseUnstructuredUpstageupstashUpTrainUSearchValtheraValyu Deep SearchVDMSVearchVectaraVectorizeVespavliteVoyageAIWeights & BiasesWeights & Biases tracingWeights & Biases trackingWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriter, Inc.xAIXataXorbits Inference (Xinference)YahooYandexYDBYeager.aiYellowbrick01.AIYouYouTubeZepZhipu AIZillizZoteroComponentsChat modelsChat modelsAbsoAI21 LabsAlibaba Cloud PAI EASAnthropic[Deprecated] Experimental Anthropic Tools WrapperAnyscaleAzureAIChatCompletionsModelAzure OpenAIAzure ML EndpointBaichuan ChatBaidu QianfanAWS BedrockCerebrasCloudflareWorkersAICohereContextualAICoze ChatDappier AIDatabricksDeepInfraDeepSeekEden AIErnie Bot ChatEverlyAIFeatherless AIFireworksChatFriendliGoodfireGoogle GeminiGoogle Cloud Vertex AIGPTRouterGroqChatHuggingFaceIBM watsonx.aiJinaChatKineticaKonkoLiteLLMLlama 2 ChatLlama APILlamaEdgeLlama.cppmaritalkMiniMaxMistralAIMLXModelScopeMoonshotNaverNetmindNVIDIA AI EndpointsChatOCIModelDeploymentOCIGenAIChatOctoAIOllamaOpenAIOutlinesPerplexityPipeshiftChatPredictionGuardPremAIPromptLayer ChatOpenAIQwen QwQRekaRunPod Chat ModelSambaNovaCloudSambaStudioChatSeekrFlowSnowflake CortexsolarSparkLLM ChatNebula (Symbl.ai)Tencent HunyuanTogetherTongyi QwenUpstagevectaravLLM ChatVolc Engine MaasChat WriterxAIXinferenceYandexGPTChatYIYuan2.0ZHIPU AIRetrieversRetrieversActiveloop Deep MemoryAmazon KendraArceeArxivAskNewsAzure AI SearchBedrock (Knowledge Bases)BM25BoxBREEBS (Open Knowledge)ChaindeskChatGPT pluginCogneeCohere rerankerCohere RAGContextual AI RerankerDappierDocArrayDriaElasticSearch BM25ElasticsearchEmbedchainFlashRank rerankerFleet AI ContextGalaxiaGoogle DriveGoogle Vertex AI SearchGraph RAGIBM watsonx.aiJaguarDB Vector DatabaseKay.aiKinetica Vectorstore based RetrieverkNNLinkupSearchRetrieverLLMLingua Document CompressorLOTR (Merger Retriever)MetalNanoPQ (Product Quantization)needleNimbleOutlinePermitPinecone Hybrid SearchPinecone RerankPubMedQdrant Sparse VectorRAGatouilleRePhraseQueryRememberizerSEC filingSelf-querying retrieversSVMTavilySearchAPITF-IDF**NeuralDB**ValyuContextVectorizeVespaWikipediaYou.comZep CloudZep Open SourceZilliz Cloud PipelineZoteroTools/ToolkitsToolsADS4GPTsAgentQLAINetwork ToolkitAlpha VantageAmadeus ToolkitApify ActorArXivAskNewsAWS LambdaAzure AI Services ToolkitAzure Cognitive Services ToolkitAzure Container Apps dynamic sessionsShell (bash)Bearly Code InterpreterBing SearchBrave SearchBrightDataWebScraperAPIBrightDataSERPBrightDataUnlockerCassandra Database ToolkitCDPChatGPT PluginsClickUp ToolkitCogniswitch ToolkitCompass DeFi ToolkitConnery Toolkit and ToolsDall-E Image GeneratorDappierDatabricks Unity Catalog (UC)DataForSEODataheraldDuckDuckGo SearchDiscordE2B Data AnalysisEden AIElevenLabs Text2SpeechExa SearchFile SystemFinancialDatasets ToolkitFMP DataGithub ToolkitGitlab ToolkitGmail ToolkitGOATGolden QueryGoogle BooksGoogle Calendar ToolkitGoogle Cloud Text-to-SpeechGoogle DriveGoogle FinanceGoogle ImagenGoogle JobsGoogle LensGoogle PlacesGoogle ScholarGoogle SearchGoogle SerperGoogle TrendsGradioGraphQLHuggingFace Hub ToolsHuman as a toolHyperbrowser Browser Agent ToolsHyperbrowser Web Scraping ToolsIBM watsonx.aiIFTTT WebHooksInfobipIonic Shopping ToolJenkinsJina SearchJira ToolkitJSON ToolkitLemon AgentLinkupSearchToolMemgraphMemorizeMojeek SearchMultiOn ToolkitNASA ToolkitNaver SearchNuclia UnderstandingNVIDIA Riva: ASR and TTSOffice365 ToolkitOpenAPI ToolkitNatural Language API ToolkitsOpenGradientOpenWeatherMapOracle AI Vector Search: Generate SummaryOxylabsPandas DataframePassio NutritionAIPaymanAIPermitPlayWright Browser ToolkitPolygon IO Toolkit and ToolsPowerBI ToolkitPrologPubMedPython REPLReddit SearchRequests ToolkitRiza Code InterpreterRobocorp ToolkitSalesforceSceneXplainScrapeGraphSearchApiSearxNG SearchSemantic Scholar API ToolSerpAPISlack ToolkitSpark SQL ToolkitSQLDatabase ToolkitStackExchangeSteam ToolkitStripeTableauTaigaTavily ExtractTavily SearchTiloresTwilioUpstageValtheraValyuContextVectaraWikidataWikipediaWolfram AlphaWriter ToolsYahoo Finance NewsYou.com SearchYouTubeZapier Natural Language ActionsZenGuard AIDocument loadersDocument loadersacreomAgentQLLoaderAirbyteLoaderAirbyte CDK (Deprecated)Airbyte Gong (Deprecated)Airbyte Hubspot (Deprecated)Airbyte JSON (Deprecated)Airbyte Salesforce (Deprecated)Airbyte Shopify (Deprecated)Airbyte Stripe (Deprecated)Airbyte Typeform (Deprecated)Airbyte Zendesk Support (Deprecated)AirtableAlibaba Cloud MaxComputeAmazon TextractApify DatasetArcGISArxivLoaderAssemblyAI Audio TranscriptsAstraDBAsync ChromiumAsyncHtmlAthenaAWS S3 DirectoryAWS S3 FileAZLyricsAzure AI DataAzure Blob Storage ContainerAzure Blob Storage FileAzure AI Document IntelligenceBibTeXBiliBiliBlackboardBlockchainBoxBrave SearchBrowserbaseBrowserlessBSHTMLLoaderCassandraChatGPT DataCollege ConfidentialConcurrent LoaderConfluenceCoNLL-UCopy PasteCouchbaseCSVCube Semantic LayerDatadog LogsDedocDiffbotDiscordDoclingDocugamiDocusaurusDropboxDuckDBEmailEPubEtherscanEverNoteexample_dataFacebook ChatFaunaFigmaFireCrawlGeopandasGitGitBookGitHubGlue CatalogGoogle AlloyDB for PostgreSQLGoogle BigQueryGoogle BigtableGoogle Cloud SQL for SQL serverGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLGoogle Cloud Storage DirectoryGoogle Cloud Storage FileGoogle Firestore in Datastore ModeGoogle DriveGoogle El Carro for Oracle WorkloadsGoogle Firestore (Native Mode)Google Memorystore for RedisGoogle SpannerGoogle Speech-to-Text Audio TranscriptsGrobidGutenbergHacker NewsHuawei OBS DirectoryHuawei OBS FileHuggingFace datasetHyperbrowserLoaderiFixitImagesImage captionsIMSDbIuguJoplinJSONLoaderJupyter NotebookKineticalakeFSLangSmithLarkSuite (FeiShu)LLM SherpaMastodonMathPixPDFLoaderMediaWiki DumpMerge Documents LoadermhtmlMicrosoft ExcelMicrosoft OneDriveMicrosoft OneNoteMicrosoft PowerPointMicrosoft SharePointMicrosoft WordNear BlockchainModern TreasuryMongoDBNeedle Document LoaderNews URLNotion DB 2/2NucliaObsidianOpen Document Format (ODT)Open City DataOracle Autonomous DatabaseOracle AI Vector Search: Document ProcessingOrg-modePandas DataFrameparsersPDFMinerLoaderPDFPlumberPebblo Safe DocumentLoaderPolars DataFrameDell PowerScale Document LoaderPsychicPubMedPullMdLoaderPyMuPDFLoaderPyMuPDF4LLMPyPDFDirectoryLoaderPyPDFium2LoaderPyPDFLoaderPySparkQuipReadTheDocs DocumentationRecursive URLRedditRoamRocksetrspaceRSS FeedsRSTscrapflyScrapingAntSingleStoreSitemapSlackSnowflakeSource CodeSpiderSpreedlyStripeSubtitleSurrealDBTelegramTencent COS DirectoryTencent COS FileTensorFlow DatasetsTiDB2MarkdownTOMLTrelloTSVTwitterUnstructuredUnstructuredMarkdownLoaderUnstructuredPDFLoaderUpstageURLVsdxWeatherWebBaseLoaderWhatsApp ChatWikipediaUnstructuredXMLLoaderXorbits Pandas DataFrameYouTube audioYouTube transcriptsYoutubeLoaderDLYuqueZeroxPDFLoaderVector storesVector storesActiveloop Deep LakeAerospikeAlibaba Cloud OpenSearchAnalyticDBAnnoyApache DorisApertureDBAstra DB Vector StoreAtlasAwaDBAzure Cosmos DB Mongo vCoreAzure Cosmos DB No SQLAzure AI SearchBagelBagelDBBaidu Cloud ElasticSearch VectorSearchBaidu VectorDBApache CassandraChromaClarifaiClickHouseCloudflareVectorizeCouchbaseDashVectorDatabricksDingoDBDocArray HnswSearchDocArray InMemorySearchAmazon Document DBDuckDBChina Mobile ECloud ElasticSearch VectorSearchElasticsearchEpsillaFaissFaiss (Async)FalkorDBVectorStoreGelGoogle AlloyDB for PostgreSQLGoogle BigQuery Vector SearchGoogle Cloud SQL for MySQLGoogle Cloud SQL for PostgreSQLFirestoreGoogle Memorystore for RedisGoogle SpannerGoogle Vertex AI Feature StoreGoogle Vertex AI Vector SearchHippoHologresInfinispanJaguar Vector DatabaseKDB.AIKineticaLanceDBLanternLindormLLMRailsManticoreSearch VectorStoreMariaDBMarqoMeilisearchAmazon MemoryDBMilvusMomento Vector Index (MVI)MongoDB AtlasMyScaleNeo4j Vector IndexNucliaDBOceanbaseopenGaussOpenSearchOracle AI Vector Search: Vector StorePathwayPostgres EmbeddingPGVecto.rsPGVectorPineconePinecone (sparse)QdrantRedisRelytRocksetSAP HANA Cloud Vector EngineScaNNSemaDBSingleStorescikit-learnSQLiteVecSQLite-VSSSQLServerStarRocksSupabase (Postgres)SurrealDBTablestoreTairTencent Cloud VectorDBThirdAI NeuralDBTiDB VectorTigrisTileDBTimescale Vector (Postgres)TypesenseUpstash VectorUSearchValdVDMSVearchVectaraVespaviking DBvliteWeaviateXataYDBYellowbrickZepZep CloudZillizEmbedding modelsEmbedding modelsAI21Aleph AlphaAnyscaleascendAwaDBAzureOpenAIBaichuan Text EmbeddingsBaidu QianfanBedrockBGE on Hugging FaceBookend AIClarifaiCloudflare Workers AIClova EmbeddingsCohereDashScopeDatabricksDeepInfraEDEN AIElasticsearchEmbaasERNIEFake EmbeddingsFastEmbed by QdrantFireworksGoogle GeminiGoogle Vertex AIGPT4AllGradientHugging FaceIBM watsonx.aiInfinityInstruct Embeddings on Hugging FaceIPEX-LLM: Local BGE Embeddings on Intel CPUIPEX-LLM: Local BGE Embeddings on Intel GPUIntel® Extension for Transformers Quantized Text EmbeddingsJinaJohn Snow LabsLASER Language-Agnostic SEntence Representations Embeddings by Meta AILindormLlama.cppllamafileLLMRailsLocalAIMiniMaxMistralAImodel2vecModelScopeMosaicMLNaverNetmindNLP CloudNomicNVIDIA NIMsOracle Cloud Infrastructure Generative AIOllamaOpenClipOpenAIOpenVINOEmbedding Documents using Optimized and Quantized EmbeddersOracle AI Vector Search: Generate EmbeddingsOVHcloudPinecone EmbeddingsPredictionGuardEmbeddingsPremAISageMakerSambaNovaCloudSambaStudioSelf HostedSentence Transformers on Hugging FaceSolarSpaCySparkLLM Text EmbeddingsTensorFlow HubText Embeddings InferenceTextEmbed - Embedding Inference ServerTitan TakeoffTogether AIUpstageVolc EngineVoyage AIXorbits inference (Xinference)YandexGPTZhipuAIOtherComponentsDocument loadersSitemapOn this pageSitemap\\nExtends from the WebBaseLoader, SitemapLoader loads a sitemap from a given URL, and then scrapes and loads all pages in the sitemap, returning each page as a Document.\\nThe scraping is done concurrently. There are reasonable limits to concurrent requests, defaulting to 2 per second.  If you aren\\'t concerned about being a good citizen, or you control the scrapped server, or don\\'t care about load you can increase this limit. Note, while this will speed up the scraping process, it may cause the server to block you. Be careful!\\nOverview\\u200b\\nIntegration details\\u200b\\nClassPackageLocalSerializableJS supportSiteMapLoaderlangchain_community✅❌✅\\nLoader features\\u200b\\nSourceDocument Lazy LoadingNative Async SupportSiteMapLoader✅❌\\nSetup\\u200b\\nTo access SiteMap document loader you\\'ll need to install the langchain-community integration package.\\nCredentials\\u200b\\nNo credentials are needed to run this.\\nTo enable automated tracing of your model calls, set your LangSmith API key:\\n# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nInstallation\\u200b\\nInstall langchain_community.\\n%pip install -qU langchain-community\\nFix notebook asyncio bug\\u200b\\nimport nest_asyncionest_asyncio.apply()\\nInitialization\\u200b\\nNow we can instantiate our model object and load documents:\\nfrom langchain_community.document_loaders.sitemap import SitemapLoaderAPI Reference:SitemapLoader\\nsitemap_loader = SitemapLoader(web_path=\"https://api.python.langchain.com/sitemap.xml\")\\nLoad\\u200b\\ndocs = sitemap_loader.load()docs[0]\\nFetching pages: 100%|##########| 28/28 [00:04<00:00,  6.83it/s]\\nDocument(metadata={\\'source\\': \\'https://api.python.langchain.com/en/stable/\\', \\'loc\\': \\'https://api.python.langchain.com/en/stable/\\', \\'lastmod\\': \\'2024-05-15T00:29:42.163001+00:00\\', \\'changefreq\\': \\'weekly\\', \\'priority\\': \\'1\\'}, page_content=\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Python API Reference Documentation.\\\\n\\\\n\\\\nYou will be automatically redirected to the new location of this page.\\\\n\\\\n\\')\\nprint(docs[0].metadata)\\n{\\'source\\': \\'https://api.python.langchain.com/en/stable/\\', \\'loc\\': \\'https://api.python.langchain.com/en/stable/\\', \\'lastmod\\': \\'2024-05-15T00:29:42.163001+00:00\\', \\'changefreq\\': \\'weekly\\', \\'priority\\': \\'1\\'}\\nYou can change the requests_per_second parameter to increase the max concurrent requests. and use requests_kwargs to pass kwargs when send requests.\\nsitemap_loader.requests_per_second = 2# Optional: avoid `[SSL: CERTIFICATE_VERIFY_FAILED]` issuesitemap_loader.requests_kwargs = {\"verify\": False}\\nLazy Load\\u200b\\nYou can also load the pages lazily in order to minimize the memory load.\\npage = []for doc in sitemap_loader.lazy_load():    page.append(doc)    if len(page) >= 10:        # do some paged operation, e.g.        # index.upsert(page)        page = []\\nFetching pages: 100%|##########| 28/28 [00:01<00:00, 19.06it/s]\\nFiltering sitemap URLs\\u200b\\nSitemaps can be massive files, with thousands of URLs.  Often you don\\'t need every single one of them.  You can filter the URLs by passing a list of strings or regex patterns to the filter_urls parameter.  Only URLs that match one of the patterns will be loaded.\\nloader = SitemapLoader(    web_path=\"https://api.python.langchain.com/sitemap.xml\",    filter_urls=[\"https://api.python.langchain.com/en/latest\"],)documents = loader.load()\\ndocuments[0]\\nDocument(page_content=\\'\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nLangChain Python API Reference Documentation.\\\\n\\\\n\\\\nYou will be automatically redirected to the new location of this page.\\\\n\\\\n\\', metadata={\\'source\\': \\'https://api.python.langchain.com/en/latest/\\', \\'loc\\': \\'https://api.python.langchain.com/en/latest/\\', \\'lastmod\\': \\'2024-02-12T05:26:10.971077+00:00\\', \\'changefreq\\': \\'daily\\', \\'priority\\': \\'0.9\\'})\\nAdd custom scraping rules\\u200b\\nThe SitemapLoader uses beautifulsoup4 for the scraping process, and it scrapes every element on the page by default. The SitemapLoader constructor accepts a custom scraping function. This feature can be helpful to tailor the scraping process to your specific needs; for example, you might want to avoid scraping headers or navigation elements.\\nThe following example shows how to develop and use a custom function to avoid navigation and header elements.\\nImport the beautifulsoup4 library and define the custom function.\\npip install beautifulsoup4\\nfrom bs4 import BeautifulSoupdef remove_nav_and_header_elements(content: BeautifulSoup) -> str:    # Find all \\'nav\\' and \\'header\\' elements in the BeautifulSoup object    nav_elements = content.find_all(\"nav\")    header_elements = content.find_all(\"header\")    # Remove each \\'nav\\' and \\'header\\' element from the BeautifulSoup object    for element in nav_elements + header_elements:        element.decompose()    return str(content.get_text())\\nAdd your custom function to the SitemapLoader object.\\nloader = SitemapLoader(    \"https://api.python.langchain.com/sitemap.xml\",    filter_urls=[\"https://api.python.langchain.com/en/latest/\"],    parsing_function=remove_nav_and_header_elements,)\\nLocal Sitemap\\u200b\\nThe sitemap loader can also be used to load local files.\\nsitemap_loader = SitemapLoader(web_path=\"example_data/sitemap.xml\", is_local=True)docs = sitemap_loader.load()\\nAPI reference\\u200b\\nFor detailed documentation of all SiteMapLoader features and configurations head to the API reference: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.sitemap.SitemapLoader.html#langchain_community.document_loaders.sitemap.SitemapLoader\\nRelated\\u200b\\n\\nDocument loader conceptual guide\\nDocument loader how-to guides\\nEdit this pageWas this page helpful?PreviousSingleStoreNextSlackOverviewIntegration detailsLoader featuresSetupCredentialsInstallationFix notebook asyncio bugInitializationLoadLazy LoadFiltering sitemap URLsAdd custom scraping rulesLocal SitemapAPI referenceRelatedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://python.langchain.com/docs/integrations/document_loaders/sitemap/\",),)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fbae072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since an \"AI boom\" in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, DeepSeek, Copilot, Gemini, Llama, and Grok; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Technology companies developing generative AI include OpenAI, Anthropic, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions. It can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on and emulate copyrighted works of art.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since an \"AI boom\" in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, DeepSeek, Copilot, Gemini, Llama, and Grok; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Technology companies developing generative AI include OpenAI, Anthropic, Microsoft, Google, DeepSeek, and Baidu.\\nGenerative AI has raised many ethical questions. It can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on and emulate copyrighted works of art.\\nGenerative AI is used across many industries. Examples include software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe first example of an algorithmically generated media is likely the Markov chain. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\nComputers were needed to go beyond Markov chains. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural nets (2014-2019) ===\\n\\nSince inception, the field of machine learning has used both discriminative models and generative models to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress, and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\\nIn 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2, which demonstrated the ability to generalize unsupervised to many different tasks as a '),\n",
       " Document(metadata={'title': 'OpenAI', 'summary': 'OpenAI, Inc. is an American artificial intelligence (AI) organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization has a complex corporate structure. As of April 2025, it is led by the non-profit OpenAI, Inc., registered in Delaware, and has multiple for-profit subsidiaries including OpenAI Holdings, LLC and OpenAI Global, LLC. Microsoft has invested US$13 billion in OpenAI, and is entitled to 49% of OpenAI Global, LLC\\'s profits, capped at an estimated 10x their investment. Microsoft also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.', 'source': 'https://en.wikipedia.org/wiki/OpenAI'}, page_content='OpenAI, Inc. is an American artificial intelligence (AI) organization founded in December 2015 and headquartered in San Francisco, California. It aims to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization has a complex corporate structure. As of April 2025, it is led by the non-profit OpenAI, Inc., registered in Delaware, and has multiple for-profit subsidiaries including OpenAI Holdings, LLC and OpenAI Global, LLC. Microsoft has invested US$13 billion in OpenAI, and is entitled to 49% of OpenAI Global, LLC\\'s profits, capped at an estimated 10x their investment. Microsoft also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later following a reconstruction of the board. Throughout 2024, roughly half of then-employed AI safety researchers left OpenAI, citing the company\\'s prominent role in an industry-wide problem.\\n\\n\\n== History ==\\n\\n\\n=== 2015–2018: Non-profit beginnings ===\\n\\nIn December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, while YC Research never contributed any funds, Open Philanthropy contributed $30 million and another $15 million in verifiable donations were traced back to Musk. OpenAI later stated that Musk\\'s contributions totaled less than $45 million. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI was initially run from Brockman\\'s living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.\\nAccording to OpenAI\\'s charter, its founding mission is \"to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity.\"\\nAccording to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.\\nMicrosoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to thre'),\n",
       " Document(metadata={'title': 'Scale AI', 'summary': 'Scale AI, Inc. is an American artificial intelligence company based in San Francisco, California. It provides data labeling and model evaluation services to develop applications for artificial intelligence.\\nScale AI outsources data labeling through its subsidiaries, Remotasks, which focuses on computer vision and autonomous vehicles, and Outlier, which focuses on data annotation for large language models.', 'source': 'https://en.wikipedia.org/wiki/Scale_AI'}, page_content='Scale AI, Inc. is an American artificial intelligence company based in San Francisco, California. It provides data labeling and model evaluation services to develop applications for artificial intelligence.\\nScale AI outsources data labeling through its subsidiaries, Remotasks, which focuses on computer vision and autonomous vehicles, and Outlier, which focuses on data annotation for large language models.\\n\\n\\n== History ==\\nScale was founded in 2016 by Alexandr Wang and Lucy Guo. The pair previously worked together at Quora. Initial investors of Scale included Dragoneer Investment Group, Tiger Global Management and Index Ventures. In 2018, co-founder Guo left Scale \"due to differences in product vision and road map\".\\nIn August 2019, after Peter Thiel’s Founders Fund made a $100 million investment in Scale, its valuation exceeded $1 billion and it acquired Unicorn status.\\nScale contracted with the United States Department of Defense in 2020.\\nBy July 2021, Scale had reached a valuation of $7 billion, after a financing led by Greenoaks, Dragoneer Investment Group and Tiger Global Management. There was an increased demand for data labelling from clients in different industries.\\nIn February 2022, Scale AI developed its Automated Damage Identification Service in response to the Russian invasion of Ukraine. Satellite imagery was analyzed, measuring the damage to buildings, which were then geotagged and reported to humanitarian groups.\\nIn January 2023, Scale laid off 20% of its workforce.\\nIn May 2023, Scale AI signed a deal with the Army’s XVIII Airborne Corps, becoming the first AI company to deploy its large language model (known as Donovan) on a classified network.\\nIn August 2023, Scale AI’s evaluation platform was used at DEF CON, a hacking convention, at its first generative AI red team event, testing models provided by various companies.\\nIn December 2023, Scale AI was among a list of companies that contributed to Meta Platforms’s Purple Llama initiative, a security framework for the purpose of development of open generative AI models.\\nIn February 2024, Scale AI was selected by the Department of Defense to test and evaluate its large language models for military purposes under a one-year contract.\\nIn March 2024, Scale reached a valuation of almost $13 billion after Accel led another round of funding. In May 2024, Scale raised an additional $1 billion with new investors including Amazon and Meta Platforms. Its valuation reached $14 billion.\\nIn December 2024, Scale was sued by a former employee with allegations that it was committing wage theft and misclassifying workers. The following month, a second employee filed suit against the company citing similar claims. In January 2025, several contractors sued Scale alleging psychological harm from being exposed to toxic material, such as content related to violence and child abuse.\\nIn January 2025, it was reported in The Conversation that Scale AI and Meta had previously teamed up to create and sell Defense Llama, a large-language model product with military-style defense purposes. The company also took out a full-page ad in The Washington Post, appealing to American President Donald Trump to \"win the AI war\". Later in the month, Scale AI and the Center for AI Safety partnered to release Humanity\\'s Last Exam, a benchmark test for AI systems. The company has also assisted in the development of the benchmarks EnigmaEval, MultiChallenge, and MASK.\\nIn February 2025, Scale AI agreed to a five-year partnership with the Qatari government to improve government services via AI-based tools and training, including predictive analytics, automation, and advanced data analytics. The deal was signed at the Web Qatar 2025 Summit by Mohammed bin Ali bin Mohammed Al Mannai, the Qatari Minister of Communications and Information Technology. Also in February, the company became a third-party evaluator of AI models for the U.S. AI Safety Institute.\\nIn March 2025, Scale AI reached a deal with the United States D'),\n",
       " Document(metadata={'title': 'Runway (company)', 'summary': \"Runway AI, Inc. (also known as Runway and RunwayML) is an American company headquartered in New York City that specializes in generative artificial intelligence research and technologies. The company is primarily focused on creating products and models for generating videos, images, and various multimedia content. It is most notable for developing the commercial text-to-video and video generative AI models Gen-1, Gen-2, Gen-3 Alpha and Gen-4.\\nRunway's tools and AI models have been utilized in films such as Everything Everywhere All At Once, in music videos for artists including A$AP Rocky, Kanye West, Brockhampton, and The Dandy Warhols, and in editing television shows like The Late Show and Top Gear.\", 'source': 'https://en.wikipedia.org/wiki/Runway_(company)'}, page_content=\"Runway AI, Inc. (also known as Runway and RunwayML) is an American company headquartered in New York City that specializes in generative artificial intelligence research and technologies. The company is primarily focused on creating products and models for generating videos, images, and various multimedia content. It is most notable for developing the commercial text-to-video and video generative AI models Gen-1, Gen-2, Gen-3 Alpha and Gen-4.\\nRunway's tools and AI models have been utilized in films such as Everything Everywhere All At Once, in music videos for artists including A$AP Rocky, Kanye West, Brockhampton, and The Dandy Warhols, and in editing television shows like The Late Show and Top Gear.\\n\\n\\n== History ==\\nThe company was founded in 2018 by the Chileans Cristóbal Valenzuela, Alejandro Matamala and the Greek Anastasis Germanidis after they met at New York University Tisch School of the Arts ITP. The company raised US$2 million in 2018 to build a platform to deploy machine learning models at scale inside multimedia applications.\\nIn December 2020, Runway raised US$8.5 million in a Series A funding round.\\nIn December 2021, the company raised US$35 million in a Series B funding round.\\nIn August 2022, the company co-released an improved version of their Latent Diffusion Model called Stable Diffusion together with the CompVis Group at Ludwig Maximilian University of Munich and a compute donation by Stability AI.\\nOn December 21, 2022, Runway raised US$50 million in a Series C round. Followed by a $141 million Series C extension round in June 2023 at a $1.5 billion valuation from Google, Nvidia, and Salesforce to build foundational multimodal AI models for content generation to be used in films and video production.\\nIn February 2023 Runway released Gen-1 and Gen-2 the first commercial and publicly available foundational video-to-video and text-to-video generation model accessible via a simple web interface.\\nIn June 2023 Runway was selected as one of the 100 Most Influential Companies in the world by Time magazine.\\nOn 3 April 2025, Runway raised $308 million in a funding round led by General Atlantic, valuing it at over $3 billion.\\n\\n\\n== Services and technologies ==\\nRunway is focused on generative AI for video, media, and art. The company focuses on developing proprietary foundational model technology that professionals in filmmaking, post-production, advertising, editing, and visual effects can utilize. Additionally, Runway offers an iOS app aimed at consumers.\\nThe Runway product is accessible via a web platform and through an API as a managed service.\\n\\n\\n=== Stable Diffusion ===\\nStable Diffusion is an open-source deep learning, text-to-image model released in 2022 based on the original paper High-Resolution Image Synthesis with Latent Diffusion Models published by Runway and the CompVis Group at Ludwig Maximilian University of Munich. Stable Diffusion is mostly used to create images conditioned on text descriptions.\\n\\n\\n=== Gen-1 ===\\nGen-1 is a video-to-video generative AI system that synthesize new videos by applying the composition and style of an image or text prompt to the structure of a source video. The model was released in February 2023. The Gen-1 model was trained and developed by Runway based on the original paper Structure and Content-Guided Video Synthesis with Diffusion Models from Runway Research. Gen-1 is an example of generative artificial intelligence for video creation.\\n\\n\\n=== Gen-2 ===\\nGen-2 is a multimodal AI system that can generate novel videos with text, images or video clips. The model is a continuation of Gen-1 and includes a modality to generate video conditioned to text. Gen-2 is one of the first commercially available text-to-video models.\\n\\n\\n=== Gen-3 Alpha ===\\nGen-3 Alpha is the first of an upcoming series of models trained by Runway on a new infrastructure built for large-scale multimodal training. It is a major improvement in fidelity, consistency, and motion over Gen-2, and a step towards building\")]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wikipedia loader \n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Gen AI\", load_max_docs=4).load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d096bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
